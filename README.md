# Internship_Visua_Combine

Project: Build a tool for visual exploration of linguistic data

Domain: Computational Biology/Computational Linguistics

Mentor: Hunter

Description: There are two basic approaches to thinking about linguistic data.  One is formal and symbolic—language is conceived of as a sort of algebra, with discrete elements that can combine according to deterministic rules.  The other is quantitative and probabilistic—language is thought of as a set of distributions.

Both of these ways of thinking about linguistic data have their advantages and disadvantages.  When we have access to very large bodies of written language in an electronic format, the quantitative approaches can be very revealing—on a broad scale.  However, it has been difficult to get insights from those quantitative approaches on a smaller scale, and it is that smaller scale that is often needed when the goal is to understand how individual parts of a linguistic system interact.  The goal of this project is to build a tool for allowing a researcher to “drill down” into the details of a large-scale, broad-coverage linguistic data set.  You will implement Shneiderman's “Overview first, zoom and filter, then details-on-demand” approach to data visualization, applying it to the exploration of large sets of linguistic data.


Desired skills:

-	R
-	GGobi, iPlots, or Shiny
-	Basic understanding of tools for supporting reproducible research: version control, markup languages, etc.
-	Software testing
-	Object-oriented programming
-	Experimental design
-	Ability to work very independently

You should expect to begin by reading about:

-	Quantitative and symbolic approaches to representing language
-	Visual analytics
-	Corpus linguistics
-	“Large number of rare events” distributions

Miscellaneous comments:  This project requires that you develop your programming skills in two distinct areas: quantitative analysis, and graphical user interfaces.  It would be difficult to do this over the course of a short internship if you do not already have some background in one or the other.  There are some tools that support parts of this kind of analysis that I would like you to use for inspiration—the “word sketch” from the Sketch Engine web site, and various R packages that provide relatively smooth interfaces to word clouds, the tokenization tools that are needed for pre-processing, and the like.  But, putting it all together will still require quite a bit of coordination between libraries, as well as some original programming for the Sketch Engine-like functionality.
